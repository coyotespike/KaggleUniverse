{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level overview\n",
    "#### This is so tha I can get to grips with the 'shape' of the data and understand its layout. It's a little explicit but will hopefully be useful for new team member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try first 1,000 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create shortened copy of data--I also experienced much crashing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! head -n 1000 train_1.csv > copytrain1000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of columns\n",
    "Page name\n",
    "Traffic per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Page  2015-07-01  2015-07-02\n",
      "12  All_your_base_are_belong_to_us_zh.wikipedia.or...         2.0         5.0\n",
      "13         AlphaGo_zh.wikipedia.org_all-access_spider         NaN         NaN\n",
      "14         Android_zh.wikipedia.org_all-access_spider         8.0        27.0\n",
      "15      Angelababy_zh.wikipedia.org_all-access_spider        40.0        17.0\n",
      "16           Apink_zh.wikipedia.org_all-access_spider        61.0        33.0\n"
     ]
    }
   ],
   "source": [
    "# import data into pandas datafram\n",
    "import pandas as pd\n",
    "data = pd.read_csv('copytrain1000.csv')\n",
    "# get ourselves a usable chunk with interesting names\n",
    "exploratory_chunk = data[12:17]\n",
    "# is there a neater way of doing the below? I wanted a 'range' of the columns\n",
    "print(exploratory_chunk[exploratory_chunk.columns[0:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of page name\n",
    "NameOfArticle_WhichWikiPediaProject_Type_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    All_your_base_are_belong_to_us_zh.wikipedia.or...\n",
       "13           AlphaGo_zh.wikipedia.org_all-access_spider\n",
       "14           Android_zh.wikipedia.org_all-access_spider\n",
       "15        Angelababy_zh.wikipedia.org_all-access_spider\n",
       "16             Apink_zh.wikipedia.org_all-access_spider\n",
       "Name: Page, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploratory_chunk['Page']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like the first 1,000 are all spiders. Are they likely to be a useful predictor? On the one hand they're not humans, but they are indicators of links to pages if I understand correctly. That may indirectly make them helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[page_name for page_name in data['Page'].values if 'spider' not in page_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### They're also all from Chinese Wikipedia, which scuppers a quick logistic regression on pages from en.wikipedia.org with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[page_name for page_name in data['Page'].values if 'zh.wikipedia.org' not in page_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try other pages\n",
    "Total of 145064 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   145064    145064 278011802 train_1.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc train_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 1 train_1.csv > copyTrain_chunk2.csv\n",
    "! head -n 20000 train_1.csv | tail -n 1000 >> copyTrain_chunk2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've hit Russia!\n",
    "Perhaps we could work on the tight limit by chunking by region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Иглесиас,_Энрике_ru.wikipedia.org_mobile-web_a...\n",
      "1    Николай_Чудотворец_ru.wikipedia.org_mobile-web...\n",
      "2        Канада_ru.wikipedia.org_mobile-web_all-agents\n",
      "3    Гумилёв,_Николай_Степанович_ru.wikipedia.org_m...\n",
      "4    Жуковский,_Василий_Андреевич_ru.wikipedia.org_...\n",
      "Name: Page, dtype: object\n"
     ]
    }
   ],
   "source": [
    "chunk2 = pd.read_csv('copyTrain_chunk2.csv')\n",
    "print(chunk2.head()['Page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
